{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5867f0-f560-4318-9ac0-e022b649b471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:10:22.684189700Z",
     "start_time": "2025-03-28T19:10:22.607972200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mapeo de etiquetas: {0: 'sin sarcasm', 1: 'sarcasm'}\n",
      "Conjunto de entrenamiento: 16026 (0: 8363, 1: 7663)\n",
      "Conjunto de entrenamiento: 4007 (0: 2116, 1: 1891)\n",
      "Conjunto de entrenamiento: 8586 (0: 4506, 1: 4080)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 读取讽刺数据集\n",
    "def read_sarcasm_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['Y'].tolist()  # 0 -> sin sarcasm, 1 -> sarcasm\n",
    "    return texts, labels\n",
    "\n",
    "# 数据路径\n",
    "train_file = \"D:/Data/sarcasm/sarcasm_train.csv\"\n",
    "test_file = \"D:/Data/sarcasm/sarcasm_test.csv\"\n",
    "\n",
    "# 读取数据\n",
    "train_texts, train_labels = read_sarcasm_data(train_file)\n",
    "test_texts, test_labels = read_sarcasm_data(test_file)\n",
    "\n",
    "# 分割训练集和验证集\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# 输出数据集大小\n",
    "print(\"\\nMapeo de etiquetas: {0: 'sin sarcasm', 1: 'sarcasm'}\")\n",
    "print(f\"Conjunto de entrenamiento: {len(train_texts)} (0: {train_labels.count(0)}, 1: {train_labels.count(1)})\")\n",
    "print(f\"Conjunto de entrenamiento: {len(val_texts)} (0: {val_labels.count(0)}, 1: {val_labels.count(1)})\")\n",
    "print(f\"Conjunto de entrenamiento: {len(test_texts)} (0: {test_labels.count(0)}, 1: {test_labels.count(1)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34a24d4ffc019975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:34:27.890104100Z",
     "start_time": "2025-03-28T19:34:16.633354300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold found: 0.10\n",
      "Optimized VADER Results (threshold=0.10):\n",
      "Accuracy: 0.6358 | Precision: 0.6180 | Recall: 0.6118 | F1-Score: 0.6149\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "# 初始化VADER\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# === 1. 自定义讽刺词典（扩展）===\n",
    "## 新增动态词典更新（基于训练数据）\n",
    "def update_vader_lexicon(train_texts, train_labels):\n",
    "    from collections import defaultdict\n",
    "    sarcasm_counts = defaultdict(int)\n",
    "    non_sarcasm_counts = defaultdict(int)\n",
    "    \n",
    "    for text, label in zip(train_texts, train_labels):\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        for token in tokens:\n",
    "            if label == 1:\n",
    "                sarcasm_counts[token] += 1\n",
    "            else:\n",
    "                non_sarcasm_counts[token] += 1\n",
    "                \n",
    "    for word in sarcasm_counts:\n",
    "        if sarcasm_counts[word] > 2*non_sarcasm_counts[word] + 5:\n",
    "            sid.lexicon[word] = sid.lexicon.get(word, 0) + np.log(sarcasm_counts[word]+1)*0.8\n",
    "\n",
    "update_vader_lexicon(train_texts, train_labels)  # 在此插入动态更新\n",
    "\n",
    "extra_sarcasm_words = {\n",
    "    'sarcasm': 3.5, 'sarcastic': 3.0, 'irony': 2.5, 'mock': 2.0, 'pretend': 1.8,\n",
    "    'brilliant': 2.5, 'genius': 3.0, 'totally': 2.2, 'sure': 2.0, 'obviously': 2.8,\n",
    "    'yeah': 1.5, 'of_course': 3.0, 'as_if': 2.5  # 修改短语格式\n",
    "}\n",
    "sid.lexicon.update(extra_sarcasm_words)\n",
    "\n",
    "# === 2. 文本预处理 ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)       # 移除URL\n",
    "    text = re.sub(r'([!?])', r' \\1 ', text)   # 保留并隔离!?符号\n",
    "    text = re.sub(r'[^a-zA-Z!? ]', '', text)  # 保留字母和!? \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = mark_negation(tokens)            # 使用NLTK否定处理\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 重新清洗所有数据集\n",
    "cleaned_train = [preprocess(t) for t in train_texts]\n",
    "cleaned_val = [preprocess(t) for t in val_texts]\n",
    "cleaned_test = [preprocess(t) for t in test_texts]\n",
    "\n",
    "# === 3. 计算讽刺得分（优化版）===\n",
    "def calculate_sarcasm_score(text):\n",
    "    scores = sid.polarity_scores(text)\n",
    "    # 新增情感冲突检测和符号加权\n",
    "    conflict = max(scores['pos'], scores['neg']) - min(scores['pos'], scores['neg'])\n",
    "    punctuation_bonus = text.count('!')*0.15 + text.count('?')*0.1\n",
    "    return conflict * (1 - scores['neu']) + punctuation_bonus\n",
    "\n",
    "# === 4. 动态优化阈值 ===\n",
    "def find_best_threshold(y_true, texts):\n",
    "    thresholds = np.arange(0.1, 1.0, 0.05)  # 扩大搜索范围\n",
    "    best_f1, best_thresh = 0, 0.5\n",
    "    for t in thresholds:\n",
    "        preds = [1 if calculate_sarcasm_score(text) >= t else 0 for text in texts]\n",
    "        current_f1 = f1_score(y_true, preds)\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1, best_thresh = current_f1, t\n",
    "    return best_thresh\n",
    "\n",
    "best_threshold = find_best_threshold(val_labels, cleaned_val)  # 改用验证集\n",
    "\n",
    "print(f\"Optimal threshold found: {best_threshold:.2f}\")  # 新增的打印语句\n",
    "# === 5. 进行预测 ===\n",
    "vader_preds = [1 if calculate_sarcasm_score(text) >= best_threshold else 0 for text in cleaned_test]\n",
    "\n",
    "# === 6. 计算指标 ===\n",
    "accuracy = accuracy_score(test_labels, vader_preds)\n",
    "precision = precision_score(test_labels, vader_preds)\n",
    "recall = recall_score(test_labels, vader_preds)\n",
    "f1 = f1_score(test_labels, vader_preds)\n",
    "\n",
    "print(f\"Optimized VADER Results (threshold={best_threshold:.2f}):\")\n",
    "print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ba3a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'countvectorizer__max_features': 15000, 'countvectorizer__ngram_range': (1, 2), 'multinomialnb__alpha': 1.0}\n",
      "\n",
      "Optimized BoW+NB Results:\n",
      "Accuracy: 0.7939 | Precision: 0.7829 | Recall: 0.7833 | F1-Score: 0.7831\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# === 1. 预处理优化 ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)           # 移除URL\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)       # 仅保留字母和空格\n",
    "    text = re.sub(r'\\b(not|no|never)\\b', r'\\1_', text)  # 强化否定处理\n",
    "    return text.strip()\n",
    "\n",
    "# 清洗训练/测试数据\n",
    "cleaned_train = [preprocess(t) for t in train_texts]\n",
    "cleaned_test = [preprocess(t) for t in test_texts]\n",
    "\n",
    "# === 2. 特征增强 ===\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),    # 启用bigram\n",
    "    max_features=15000,    # 扩展特征数量\n",
    "    stop_words='english',\n",
    "    binary=True   # 添加参数\n",
    ")\n",
    "\n",
    "# === 3. 超参数调优 ===\n",
    "param_grid = {\n",
    "    'multinomialnb__alpha': [0.01, 0.1, 0.5, 1.0],  # 更细粒度的平滑参数\n",
    "    'countvectorizer__ngram_range': [(1,1), (1,2)], # 专注实用ngram范围\n",
    "    'countvectorizer__max_features': [10000, 15000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    make_pipeline(vectorizer, MultinomialNB()),\n",
    "    param_grid,\n",
    "    cv=5,                  # 增加交叉验证稳定性\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 使用清洗后的数据训练\n",
    "grid.fit(cleaned_train, train_labels)\n",
    "\n",
    "# 最优参数\n",
    "print(f\"Mejores parámetros: {grid.best_params_}\")\n",
    "\n",
    "# === 4. 最终评估 ===\n",
    "best_model = grid.best_estimator_\n",
    "bow_nb_labels = best_model.predict(cleaned_test)\n",
    "\n",
    "accuracy_bow = accuracy_score(test_labels, bow_nb_labels)\n",
    "precision_bow = precision_score(test_labels, bow_nb_labels)\n",
    "recall_bow = recall_score(test_labels, bow_nb_labels)\n",
    "f1_bow = f1_score(test_labels, bow_nb_labels)\n",
    "\n",
    "print(\"\\nOptimized BoW+NB Results:\")\n",
    "print(f\"Accuracy: {accuracy_bow:.4f} | Precision: {precision_bow:.4f} | Recall: {recall_bow:.4f} | F1-Score: {f1_bow:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5daaff5f1fa0c98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:57:25.319042200Z",
     "start_time": "2025-03-28T19:57:20.552318100Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'countvectorizer__max_features': 15000, 'countvectorizer__ngram_range': (1, 2), 'multinomialnb__alpha': 1.0}\n",
      "\n",
      "Optimized BoW+NB Results:\n",
      "Accuracy: 0.7935 | Precision: 0.7833 | Recall: 0.7816 | F1-Score: 0.7825\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# === 1. 预处理优化 ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)           # 移除URL\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)       # 仅保留字母和空格\n",
    "    text = re.sub(r'\\b(not|no|never)\\b', r'\\1_', text)  # 强化否定处理\n",
    "    return text.strip()\n",
    "\n",
    "# 清洗训练/测试数据\n",
    "cleaned_train = [preprocess(t) for t in train_texts]\n",
    "cleaned_test = [preprocess(t) for t in test_texts]\n",
    "\n",
    "# === 2. 特征增强 ===\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=(1, 2),    # 启用bigram\n",
    "    max_features=15000,    # 扩展特征数量\n",
    "    stop_words='english'\n",
    "    \n",
    ")\n",
    "\n",
    "# === 3. 超参数调优 ===\n",
    "param_grid = {\n",
    "    'multinomialnb__alpha': [0.01, 0.1, 0.5, 1.0],  # 更细粒度的平滑参数\n",
    "    'countvectorizer__ngram_range': [(1,1), (1,2)], # 专注实用ngram范围\n",
    "    'countvectorizer__max_features': [10000, 15000]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    make_pipeline(vectorizer, MultinomialNB()),\n",
    "    param_grid,\n",
    "    cv=5,                  # 增加交叉验证稳定性\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 使用清洗后的数据训练\n",
    "grid.fit(cleaned_train, train_labels)\n",
    "\n",
    "# 最优参数\n",
    "print(f\"Mejores parámetros: {grid.best_params_}\")\n",
    "\n",
    "# === 4. 最终评估 ===\n",
    "best_model = grid.best_estimator_\n",
    "bow_nb_labels = best_model.predict(cleaned_test)\n",
    "\n",
    "accuracy_bow = accuracy_score(test_labels, bow_nb_labels)\n",
    "precision_bow = precision_score(test_labels, bow_nb_labels)\n",
    "recall_bow = recall_score(test_labels, bow_nb_labels)\n",
    "f1_bow = f1_score(test_labels, bow_nb_labels)\n",
    "\n",
    "print(\"\\nOptimized BoW+NB Results:\")\n",
    "print(f\"Accuracy: {accuracy_bow:.4f} | Precision: {precision_bow:.4f} | Recall: {recall_bow:.4f} | F1-Score: {f1_bow:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3497b410d62e756",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-28T19:40:57.752644400Z",
     "start_time": "2025-03-28T19:40:52.627072300Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros encontrados: {'linearsvc__C': 0.1, 'tfidfvectorizer__max_features': 20000, 'tfidfvectorizer__ngram_range': (1, 2)}\n",
      "\n",
      "TF-IDF+SVM Optimizado para Detección de Ironía:\n",
      "Accuracy: 0.8403 | Precision: 0.8151 | Recall: 0.8588 | F1-Score: 0.8364\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# === 1. 预处理优化 ===\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)          # 移除URL（社交媒体特性）\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)      # 仅保留字母\n",
    "    text = re.sub(r'\\b(not|no|never)\\b\\s*', r'\\1_', text)  # 处理否定结构（生成\"not_good\"等特征）\n",
    "    return text.strip()\n",
    "\n",
    "# 清洗训练/测试数据\n",
    "cleaned_train_texts = [preprocess(text) for text in train_texts]\n",
    "cleaned_test_texts = [preprocess(text) for text in test_texts]\n",
    "\n",
    "# === 2. 构建TF-IDF + SVM管道 ===\n",
    "tfidf = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),    # 捕捉tri-gram特征（如\"love_waiting_in_line\"）\n",
    "    max_features=20000,    # 扩展特征数量\n",
    "    stop_words=None        # 保留停用词（否定词如\"not\"对讽刺检测很重要）\n",
    ")\n",
    "\n",
    "svm = LinearSVC(\n",
    "    class_weight='balanced',  # 处理类别不平衡\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 构建管道\n",
    "tfidf_svm_model = make_pipeline(tfidf, svm)\n",
    "\n",
    "# === 3. 超参数调优 ===\n",
    "param_grid = {\n",
    "    'tfidfvectorizer__ngram_range': [(1,2), (1,3)],  # 优化n-gram范围\n",
    "    'tfidfvectorizer__max_features': [10000, 20000],\n",
    "    'linearsvc__C': [0.1, 1, 10]  # 调整正则化强度\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    tfidf_svm_model,\n",
    "    param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 使用清洗后的数据训练\n",
    "grid.fit(cleaned_train_texts, train_labels)\n",
    "\n",
    "# 最优参数\n",
    "print(f\"Mejores parámetros encontrados: {grid.best_params_}\")\n",
    "\n",
    "# === 4. 最终模型训练 ===\n",
    "best_model = grid.best_estimator_\n",
    "tfidf_svm_labels = best_model.predict(cleaned_test_texts)\n",
    "\n",
    "# === 5. 性能评估 ===\n",
    "accuracy = accuracy_score(test_labels, tfidf_svm_labels)\n",
    "precision = precision_score(test_labels, tfidf_svm_labels)\n",
    "recall = recall_score(test_labels, tfidf_svm_labels)\n",
    "f1 = f1_score(test_labels, tfidf_svm_labels)\n",
    "\n",
    "print(f\"\\nTF-IDF+SVM Optimizado para Detección de Ironía:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
