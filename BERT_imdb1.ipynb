{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wk_8JASk54Zj",
        "outputId": "b5de2fcf-02c2-4073-8c00-78fd5cb609fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odE4w-fgGNn_",
        "outputId": "4890e780-f619-41a7-c366-9dcb6d071169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uso de datos persistentes en Google Drive\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 配置路径\n",
        "DRIVE_DATA_PATH = '/content/drive/MyDrive/Colab Notebooks/data'  # Google Drive存储路径\n",
        "\n",
        "# 文件路径配置\n",
        "file_paths = {\n",
        "    'train': ('imdb_train.npz', ['x', 'y']),\n",
        "    'test': ('imdb_test.npz', ['x', 'y']),\n",
        "    'val': ('imdb_val.npz', ['x', 'y'])\n",
        "}\n",
        "\n",
        "# 自动检测数据存在路径\n",
        "def check_data_source():\n",
        "    # 检查Google Drive是否有数据\n",
        "    drive_files_exist = all([os.path.exists(os.path.join(DRIVE_DATA_PATH, f[0])) for f in file_paths.values()])\n",
        "\n",
        "    if drive_files_exist:\n",
        "        print(\"Uso de datos persistentes en Google Drive\")\n",
        "        return DRIVE_DATA_PATH\n",
        "\n",
        "    # 如果没有找到数据，提示用户上传\n",
        "    print(\"\"\"\n",
        "    ¡Archivo de datos no encontrado!\n",
        "    Por favor seleccione una fuente de datos:\n",
        "    1. Subir desde local a Google Drive (recomendado)\n",
        "    \"\"\")\n",
        "    from google.colab import files\n",
        "    for file_info in file_paths.values():\n",
        "        print(f\"Subiendo {file_info[0]}...\")\n",
        "        uploaded = files.upload()\n",
        "        for fn in uploaded.keys():\n",
        "            os.rename(fn, os.path.join(DRIVE_DATA_PATH, fn))\n",
        "    return DRIVE_DATA_PATH\n",
        "\n",
        "# 确定数据源\n",
        "DATA_SOURCE = check_data_source()\n",
        "\n",
        "# 模型参数配置\n",
        "max_length = 200\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5\n",
        "number_of_epochs = 10\n",
        "bert_weight_name = 'bert-base-uncased'\n",
        "\n",
        "# 初始化分词器\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_weight_name)\n",
        "\n",
        "# 数据加载函数\n",
        "def get_raw_dataset():\n",
        "    def load_data(file_name, keys):\n",
        "        path = os.path.join(DATA_SOURCE, file_name)\n",
        "        if not os.path.exists(path):\n",
        "            raise FileNotFoundError(f\"Archivos de datos {path} No existe, verifique si la ruta y el nombre del archivo son correctos\")\n",
        "        data = np.load(path)\n",
        "        return tuple(data[key] for key in keys)\n",
        "\n",
        "    try:\n",
        "        return (\n",
        "            *load_data(file_paths['train'][0], file_paths['train'][1]),\n",
        "            *load_data(file_paths['test'][0], file_paths['test'][1]),\n",
        "            *load_data(file_paths['val'][0], file_paths['val'][1])\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la carga de datos：{e}\")\n",
        "        raise\n",
        "\n",
        "# 数据预处理函数\n",
        "def convert_example_to_feature(review):\n",
        "    return tokenizer.encode_plus(\n",
        "        review,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        pad_to_max_length=True,\n",
        "        return_attention_mask=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"token_type_ids\": token_type_ids,\n",
        "        \"attention_mask\": attention_masks\n",
        "    }, label\n",
        "\n",
        "def encode_examples(x, y, limit=-1):\n",
        "    input_ids_list = []\n",
        "    token_type_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    label_list = []\n",
        "\n",
        "    for review, label in zip(x[:limit], y[:limit]):\n",
        "        bert_input = convert_example_to_feature(review)\n",
        "        input_ids_list.append(bert_input['input_ids'])\n",
        "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
        "        attention_mask_list.append(bert_input['attention_mask'])\n",
        "        label_list.append([label])\n",
        "\n",
        "    return tf.data.Dataset.from_tensor_slices(\n",
        "        (input_ids_list, attention_mask_list, token_type_ids_list, label_list)\n",
        "    ).map(map_example_to_dict)\n",
        "\n",
        "# 数据集生成\n",
        "def get_dataset():\n",
        "    x_train, y_train, x_test, y_test, x_val, y_val = get_raw_dataset()\n",
        "\n",
        "    ds_train_encoded = encode_examples(x_train, y_train).shuffle(10000).batch(batch_size)\n",
        "    ds_test_encoded = encode_examples(x_test, y_test).batch(batch_size)\n",
        "    ds_val_encoded = encode_examples(x_val, y_val).batch(batch_size)\n",
        "\n",
        "    return ds_train_encoded, ds_test_encoded, ds_val_encoded\n",
        "\n",
        "# 模型构建\n",
        "def build_model():\n",
        "    model = TFBertForSequenceClassification.from_pretrained(bert_weight_name)\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugsHDGi1GVCS",
        "outputId": "33ec9f01-6c51-49c8-d282-b3ae631f2c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "初始化模型...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
            "\n",
            "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"tf_bert_for_sequence_classification_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bert (TFBertMainLayer)      multiple                  109482240 \n",
            "                                                                 \n",
            " dropout_113 (Dropout)       multiple                  0 (unused)\n",
            "                                                                 \n",
            " classifier (Dense)          multiple                  1538      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 109483778 (417.65 MB)\n",
            "Trainable params: 109483778 (417.65 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# 第一部分：初始化模型\n",
        "print(\"\\nInicializar el modelo...\")\n",
        "model = build_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYEQwJCsGzh-",
        "outputId": "8727cb7b-c6a4-4eb4-fb3c-0e9f6ac14ff2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Carga de conjuntos de datos ...\n"
          ]
        }
      ],
      "source": [
        "# 第二部分：加载数据集\n",
        "print(\"\\nCarga de conjuntos de datos ...\")\n",
        "train_data, test_data, val_data = get_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vR2md_nG9PN",
        "outputId": "086b076e-8d2d-44bf-aa1d-e1a51894480e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "开始训练...\n",
            "Epoch 1/10\n",
            "1250/1250 [==============================] - 1015s 781ms/step - loss: 0.2914 - accuracy: 0.8775 - val_loss: 0.2409 - val_accuracy: 0.9064\n",
            "Epoch 2/10\n",
            "1250/1250 [==============================] - 980s 784ms/step - loss: 0.1618 - accuracy: 0.9404 - val_loss: 0.2767 - val_accuracy: 0.8980\n",
            "Epoch 3/10\n",
            "1250/1250 [==============================] - 980s 784ms/step - loss: 0.0804 - accuracy: 0.9725 - val_loss: 0.2953 - val_accuracy: 0.9006\n",
            "Epoch 4/10\n",
            "1250/1250 [==============================] - 973s 778ms/step - loss: 0.0469 - accuracy: 0.9857 - val_loss: 0.3672 - val_accuracy: 0.9040\n",
            "Epoch 5/10\n",
            "1250/1250 [==============================] - 970s 776ms/step - loss: 0.0331 - accuracy: 0.9901 - val_loss: 0.3611 - val_accuracy: 0.9012\n",
            "Epoch 6/10\n",
            "1250/1250 [==============================] - 979s 783ms/step - loss: 0.0284 - accuracy: 0.9908 - val_loss: 0.4304 - val_accuracy: 0.8964\n",
            "Epoch 7/10\n",
            "1250/1250 [==============================] - 980s 784ms/step - loss: 0.0256 - accuracy: 0.9922 - val_loss: 0.4370 - val_accuracy: 0.8950\n",
            "Epoch 8/10\n",
            "1250/1250 [==============================] - 979s 783ms/step - loss: 0.0195 - accuracy: 0.9934 - val_loss: 0.5956 - val_accuracy: 0.8720\n",
            "Epoch 9/10\n",
            "1250/1250 [==============================] - 972s 777ms/step - loss: 0.0179 - accuracy: 0.9944 - val_loss: 0.4243 - val_accuracy: 0.8994\n",
            "Epoch 10/10\n",
            "1250/1250 [==============================] - 978s 782ms/step - loss: 0.0132 - accuracy: 0.9960 - val_loss: 0.5008 - val_accuracy: 0.9018\n"
          ]
        }
      ],
      "source": [
        "# 第三部分：训练模型\n",
        "print(\"\\nEmpezar a entrenar...\")\n",
        "history = model.fit(\n",
        "    train_data,\n",
        "    validation_data=val_data,\n",
        "    epochs=number_of_epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp_yA7S5Ipn_",
        "outputId": "cd0f0f78-b92a-4bb6-ccc1-465ec15d0a20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型已保存到 /content/drive/MyDrive/Colab Notebooks/model/saved_model\n"
          ]
        }
      ],
      "source": [
        "# 第四部分：保存模型（可选）\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/model/saved_model'\n",
        "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)  # 确保目录存在\n",
        "model.save_pretrained(model_save_path)\n",
        "print(f\"Modelo guardado en {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx07W5zXIvb4",
        "outputId": "0f74fc36-4b72-4242-829b-4f3459f3257c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model/saved_model were not used when initializing TFBertForSequenceClassification: ['dropout_113']\n",
            "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at /content/drive/MyDrive/Colab Notebooks/model/saved_model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado desde archivo\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo (si es necesario desde un archivo)\n",
        "from transformers import TFBertForSequenceClassification\n",
        "model_load_path = '/content/drive/MyDrive/Colab Notebooks/model/saved_model'\n",
        "model = TFBertForSequenceClassification.from_pretrained(model_load_path)\n",
        "print(\"Modelo cargado desde archivo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCoUmIaquEKG"
      },
      "outputs": [],
      "source": [
        "# Recompilar el modelo\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zPuBoTmKdhJ",
        "outputId": "65b45e35-2ea3-4bb5-e137-1afa36e88fd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluación final：\n",
            "Loss: 0.4820\n",
            "Accuracy: 0.9064\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nEvaluación final：\")\n",
        "loss, acc = model.evaluate(test_data, verbose=0)\n",
        "\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5geKupKzKhl4",
        "outputId": "c07d2d57-b522-49a3-95db-4493065d50f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.9065\n",
            "Recall: 0.9064\n",
            "F1 Score: 0.9063\n"
          ]
        }
      ],
      "source": [
        "# 第七部分：评估模型 - 计算精确率、召回率和 F1 分数\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for batch in test_data:\n",
        "    inputs, labels = batch\n",
        "    predictions = model(inputs).logits\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(tf.argmax(predictions, axis=1).numpy())\n",
        "\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Accuracy: {acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
