{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvX9DtGvFXlc",
    "outputId": "1dfef802-5dcf-4626-96f6-a25d4e406178"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:17.063945Z",
     "iopub.status.busy": "2025-04-03T14:32:17.063471Z",
     "iopub.status.idle": "2025-04-03T14:32:17.411746Z",
     "shell.execute_reply": "2025-04-03T14:32:17.411193Z",
     "shell.execute_reply.started": "2025-04-03T14:32:17.063912Z"
    },
    "id": "8PzELvdPE_iQ",
    "outputId": "681e1384-24aa-4d10-d28c-ee5f6806acde",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conjunto de entrenamiento: 16026 | conjunto de validaci√≥n: 4007 | conjunto de pruebas: 8586\n"
     ]
    }
   ],
   "source": [
    "# === 3. ÂØºÂÖ•Ê®°Âùó ===\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "# os.environ['WANDB_API_KEY'] = '255fab36462f5587d825c69b9d5b53a852a2c4d3'  # ÊõøÊç¢‰∏∫‰Ω†ÁöÑÂÆûÈôÖ API ÂØÜÈí•\n",
    "os.environ['WANDB_MODE'] = 'disabled'\n",
    "\n",
    "# === 4. Êï∞ÊçÆÂä†ËΩΩ‰∏éÈ¢ÑÂ§ÑÁêÜ ===\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['text'].tolist()\n",
    "    labels = df['Y'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Ë∑ØÂæÑÈÖçÁΩÆÔºàËØ∑Á°Æ‰øùÊñá‰ª∂Â∑≤‰∏ä‰º†Âà∞Google DriveÔºâ\n",
    "train_file = \"/content/drive/MyDrive/Colab Notebooks/sarcasm/sarcasm_train.csv\"\n",
    "test_file = \"/content/drive/MyDrive/Colab Notebooks/sarcasm/sarcasm_test.csv\"\n",
    "\n",
    "# Âä†ËΩΩÊï∞ÊçÆ\n",
    "train_texts, train_labels = load_data(train_file)\n",
    "test_texts, test_labels = load_data(test_file)\n",
    "\n",
    "# ÂàÜÂâ≤ËÆ≠ÁªÉÈõÜ/È™åËØÅÈõÜ\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    train_texts, train_labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"conjunto de entrenamiento: {len(train_texts)} | conjunto de validaci√≥n: {len(val_texts)} | conjunto de pruebas: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371,
     "referenced_widgets": [
      "66f8166e64914aebb792e9a2e67a1301",
      "ba713d97edb94f2ba375a5eea74dbf93",
      "d4e9f339f6ad48509922aaadbab05cdc",
      "3b96dd43a69147c5bf4865aac51d1e26",
      "0fb405e66c514420a80d0dfdad9c6c16",
      "f919dec1d35c4c38bed235844dd24ff2",
      "ad79efdfbb484efe90d8c4bb10c9da04",
      "3b86a128d730485ab2b233798a30abbe",
      "d7979179b1524ef1aae454c723954b94",
      "23886ba4f0ee42969a08a3f1529267ea",
      "a7da3b26af144dafab25384c9972275f",
      "f5e45b89cd224909b1435ca4c752e80e",
      "06ca16918f664b7480cbdf81dfb56bd3",
      "0ff9d731e18d4fcba58eb55af61143bb",
      "fe7bffc861c04f20a13d8648326cf3fe",
      "f95d6f4499274fb2888af58398d845b3",
      "9000a97c925f414eaa50986597ff282d",
      "d7faa11e441b45ce98320c9cfa84ac26",
      "ff54d8580f2847fb95cc3e8989afbeba",
      "7b373d3992c14075b09c56b44cc7523c",
      "0448c2fd7dac4506aa682cb9100bff3b",
      "611553804645450bb879cff5e6a31214",
      "f8a2b67a950042f59682b95d0e9dfacf",
      "14f2ba3a0208439b84e3bd9daa7b92ec",
      "4dd5fc885a104b2da4c6aea7eacd7bf7",
      "6329495e4ac4482ea161fa07fa00f24c",
      "de520754853448b8a2c215e62b6baa9f",
      "0ddb22a4bb414ffdb3314311bdcbd554",
      "f378f883fbe2482ab49af0549976865a",
      "c4fe05a9225c46d8bb56041f9ca93a5f",
      "1319a944c0f04189bb6a8d699afdc162",
      "5a634c52fd8e493bad3ad26128ae6526",
      "336fcf601a9d416cb47d94fbf14fba64",
      "8414015222cd4720a48d99e62f4befbb",
      "4c85f37d5d5c40adbc32d9d8bb132d8d",
      "17afb7853d46471b9fa6e74447eb2be0",
      "d513ad64fc19436596b1a606c4051b8d",
      "e9b276c3c09d41349391774e6a09b865",
      "f6a777f7deb54efd80a87ac8c7ace241",
      "477f0bf9d5a845039bb705abc63d919f",
      "3d6c8e46cca542858949f10abb60f776",
      "2971a830049442cf8cbae86605406559",
      "2e4f9dbf266b4caa9eb056299bb1d345",
      "b14759c3e7924ce2a7c2637f59fad594",
      "932023af60ee47829c1202567f4f7c61",
      "828bb80f6c9f4ee2ac9ea2b22383ece6",
      "889663e74903468c8d39ec5e6c4375be",
      "73b7bcfe1f774c1ab5d747d7a3a2236c",
      "2b0596c622ee4360923df4d4845fe9e5",
      "fc82a7d3572248778b7501ee57e803d5",
      "d0b971da5a4544058b24e2d5f260de7a",
      "6d4ba3ab1a2a4c03901527847d2a20e3",
      "03e24b511ab7461d8a5d367c743e561f",
      "82af01f5f84b4f17b28e7b906a5bddde",
      "e9bb4f3078cf4e73b2630ecab3f493c7"
     ]
    },
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-03T14:32:23.282184Z",
     "iopub.status.busy": "2025-04-03T14:32:23.281860Z",
     "iopub.status.idle": "2025-04-03T14:32:23.285122Z",
     "shell.execute_reply": "2025-04-03T14:32:23.284676Z",
     "shell.execute_reply.started": "2025-04-03T14:32:23.282164Z"
    },
    "id": "Nw1S4dS7E_iR",
    "outputId": "0576fced-5331-4c4d-9c73-4b6e5958df9f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f8166e64914aebb792e9a2e67a1301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e45b89cd224909b1435ca4c752e80e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a2b67a950042f59682b95d0e9dfacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8414015222cd4720a48d99e62f4befbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932023af60ee47829c1202567f4f7c61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "import torch\n",
    "\n",
    "# === 5. BERT‰∏ìÁî®È¢ÑÂ§ÑÁêÜ ===\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)          # ÁßªÈô§URL\n",
    "    text = re.sub(r'\\b(not|no|never)\\b\\s*', r'\\1_', text)  # Â§ÑÁêÜÂê¶ÂÆöËØç\n",
    "    return text.strip()\n",
    "\n",
    "# Ê∏ÖÊ¥óÊï∞ÊçÆ\n",
    "cleaned_train = [preprocess(text) for text in train_texts]\n",
    "cleaned_val = [preprocess(text) for text in val_texts]\n",
    "cleaned_test = [preprocess(text) for text in test_texts]\n",
    "\n",
    "# ËΩ¨Êç¢‰∏∫DatasetÊ†ºÂºè\n",
    "class SarcasmDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=128,  # ËÆΩÂà∫ÊñáÊú¨ÈÄöÂ∏∏ËæÉÁü≠\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: v[idx] for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SarcasmDataset(cleaned_train, train_labels)\n",
    "val_dataset = SarcasmDataset(cleaned_val, val_labels)\n",
    "test_dataset = SarcasmDataset(cleaned_test, test_labels)\n",
    "\n",
    "# === 6. Ê®°ÂûãÈÖçÁΩÆ‰∏éËÆ≠ÁªÉ ===\n",
    "model_save_path = \"/content/drive/MyDrive/Colab Notebooks/model/sarcasm\"\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "# Âä†ËΩΩÈ¢ÑËÆ≠ÁªÉÊ®°Âûã\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=2,\n",
    "    attention_probs_dropout_prob=0.1  # Â¢ûÂä†DropoutÈò≤Ê≠¢ËøáÊãüÂêà\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "vFJEyFS2E_iS",
    "outputId": "982d6106-3eb3-4c49-a234-dcd010734e85",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10020' max='10020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10020/10020 33:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.291900</td>\n",
       "      <td>0.286147</td>\n",
       "      <td>0.895433</td>\n",
       "      <td>0.920091</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.884985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.204000</td>\n",
       "      <td>0.354835</td>\n",
       "      <td>0.907162</td>\n",
       "      <td>0.894956</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>0.902465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.057500</td>\n",
       "      <td>0.519682</td>\n",
       "      <td>0.913651</td>\n",
       "      <td>0.922362</td>\n",
       "      <td>0.892121</td>\n",
       "      <td>0.906989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.573443</td>\n",
       "      <td>0.918642</td>\n",
       "      <td>0.906072</td>\n",
       "      <td>0.923321</td>\n",
       "      <td>0.914615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.644421</td>\n",
       "      <td>0.918393</td>\n",
       "      <td>0.918630</td>\n",
       "      <td>0.907456</td>\n",
       "      <td>0.913009</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BERT Results for Sarcasm Detection:\n",
      "Accuracy: 0.9115 | Precision: 0.9031 | Recall: 0.9115 | F1-Score: 0.9073\n"
     ]
    }
   ],
   "source": [
    "# ËÆ≠ÁªÉÂèÇÊï∞\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,          # Êõ¥È´òÁöÑÂ≠¶‰π†Áéá\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=5,          # Â¢ûÂä†ËÆ≠ÁªÉËΩÆÊ¨°\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True, # Ê†πÊçÆÈ™åËØÅÈõÜÂä†ËΩΩÊúÄ‰ºòÊ®°Âûã\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    return {'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
    "\n",
    "\n",
    "# Ê∑ªÂä†Êó©ÂÅúÊú∫Âà∂\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# ÊâßË°åËÆ≠ÁªÉ\n",
    "trainer.train()\n",
    "\n",
    "# ‰øùÂ≠òÊúÄ‰Ω≥Ê®°Âûã\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# === 7. Ê®°ÂûãËØÑ‰º∞ ===\n",
    "predictions = trainer.predict(test_dataset)\n",
    "bert_preds = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# ËÆ°ÁÆóÊåáÊ†á\n",
    "accuracy = accuracy_score(test_labels, bert_preds)\n",
    "precision = precision_score(test_labels, bert_preds)\n",
    "recall = recall_score(test_labels, bert_preds)\n",
    "f1 = f1_score(test_labels, bert_preds)\n",
    "\n",
    "print(f\"\\nBERT Results for Sarcasm Detection:\")\n",
    "print(f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1-Score: {f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
